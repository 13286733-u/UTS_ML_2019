{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/13286733-u/UTS_ML_2019/blob/master/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFcJZl65KXDY",
        "colab_type": "text"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "\n",
        "\n",
        "The Decision Tree algorithm is one of the most well-known machine learning algorithms available. The decision tree, as stated in the name, uses a tree-like model to determine specific outcomes or decisions. Decision trees are also a “greedy” algorithm, which means that it attempts to make the most locally optimum choice at each stage. Although this algorithm can is useful for both classification and regression models, this report will mainly focus on the implementation of a classification decision tree. In this report, I will cover a basic implementation of the ID3 decision tree, or otherwise known as the “Iterative Dichotomiser 3” decision tree. This type of decision tree was invented by Ross Quinlan and is the processor to the C4.5 decision tree. When using the ID3 decision tree as a classifier, input data can be either numerical or categorical, and the outcome is usually used to determine a category or Boolean value.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GFqM9mzX428n",
        "colab_type": "text"
      },
      "source": [
        "# Exploration\n",
        "\n",
        "\n",
        "Although I have had previous experience in programming, I was not entirely prepared to implement a machine learning algorithm from scratch. Furthermore, all my previous experience around programming was web or app-based, which was little help for this project which was my first challenge. To correctly implement the ID3 algorithm, I would have to skim through the Python documentation and teach myself throughout this project. The next problem was the actual implementation. Even though I had a rough understanding of how the ID3 decision tree worked, it wasn’t as straight forward to implement it in code.\n",
        " \n",
        "To properly implement the ID3 decision tree, two metrics would need to be calculated and utilized: Entropy and Information Gain. \n",
        "\n",
        "Entropy is a thermodynamic quantity used to measure importance relative to its size which uses the formulas below.\n",
        "\n",
        "![Wikipedia Entropy Description](https://miro.medium.com/max/3104/1*EoWJ8bxc-iqBS-dF-XxsBA.jpeg)\n",
        "\n",
        "To visualize this we can use the target of \"playing golf\" and the below figure.\n",
        "\n",
        "![Entropy Formula In Action](https://www.saedsayad.com/images/Entropy_3.png)\n",
        "\n",
        "Information Gain, derived from entropy calculations, is used to compute the amount of information gained from a variable while observing another variable.\n",
        "\n",
        "![Wikipedia Information Gain](https://miro.medium.com/max/3162/1*wQjVzx7zCVb87htqk46vUA.jpeg)\n",
        "Source: Wikipedia\n",
        "\n",
        "These two metrics would need to be calculated for every single attribute based on their information gain weighting and used to create a decision tree.\n",
        "\n",
        "\n",
        "![Entropy with two attributes](https://www.saedsayad.com/images/Entropy_2.png)\n",
        "\n",
        "\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vKX3goK6Ndms",
        "colab_type": "text"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "To assist in implementation I have created a mockup data set as seen in the screenshot below. The iris data set has also been loaded for evaluation purposes.\n",
        "\n",
        "![Test Data Set](https://i.imgur.com/fixdTQ7.png)\n",
        "\n",
        "Along this report there will be code snippets which have been altered to run properly on Google colab. The full script has also been attached to this report and Git.\n",
        "\n",
        "To start the data needs to be loaded into an array. In the real world, certain variables need to be changed when running the algorithm.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QezpbxDKiTWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports and Setup\n",
        "# Please run this once before everything else\n",
        "\n",
        "import math\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPXahPOYRGlA",
        "colab_type": "code",
        "outputId": "cb21cf2e-0c55-470d-f978-0dd9d1bed552",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "\n",
        "# This is the same data as the picture above\n",
        "# In the real program this would be loaded via csv\n",
        "\n",
        "'''loaded_data = [ \n",
        "    [ 'Outlook', 'Temp', 'Humidity', 'Windy', 'Play' ],\n",
        "    [ 'Wet', 'Hot', 'High', False, 'NO' ],\n",
        "    [ 'Wet', 'Hot', 'High', True, 'NO' ],\n",
        "    [ 'Sunny', 'Hot', 'High', False, 'YES' ],\n",
        "    [ 'Cloudy', 'Mild', 'High', False, 'YES' ],\n",
        "    [ 'Cloudy', 'Cool', 'Normal', False, 'YES' ],\n",
        "    [ 'Cloudy', 'Cool', 'Normal', True, 'NO' ],\n",
        "    [ 'Sunny', 'Cool', 'Normal', True, 'YES' ],\n",
        "    [ 'Wet', 'Mild', 'High', False, 'NO' ],\n",
        "    [ 'Wet', 'Cool', 'Normal', False, 'YES' ],\n",
        "    [ 'Cloudy', 'Mild', 'Normal', False, 'YES' ],\n",
        "    [ 'Wet', 'Mild', 'Normal', True, 'YES' ],\n",
        "    [ 'Sunny', 'Mild', 'High', True, 'YES' ],\n",
        "    [ 'Sunny', 'Hot', 'Normal', False, 'YES' ],\n",
        "    [ 'Cloudy', 'Mild', 'High', True, 'NO' ] \n",
        "]'''\n",
        "\n",
        "# THIS IS FOR EVALUATION PURPOSES ONLY COMMENT\n",
        "# THIS OUT FOR OTHER USES\n",
        "# USE '''\n",
        "# IF YOU COMMENT THIS OUT YOU WILL NEED TO CHANGE \n",
        "# THE ATTRIBUTES AND TARGET AS WELL\n",
        "loaded_data = [\n",
        "[ 'sepal_length','sepal_width','petal_length','petal_width','species' ],\n",
        "[ 5.1, 3.5, 1.4, 0.2, 'setosa' ],\n",
        "[ 4.9, 3, 1.4, 0.2, 'setosa' ],\n",
        "[ 4.7, 3.2, 1.3, 0.2, 'setosa' ],\n",
        "[ 4.6, 3.1, 1.5, 0.2, 'setosa' ],\n",
        "[ 5, 3.6, 1.4, 0.2, 'setosa' ],\n",
        "[ 5.4, 3.9, 1.7, 0.4, 'setosa' ],\n",
        "[ 4.6, 3.4, 1.4, 0.3, 'setosa' ],\n",
        "[ 5, 3.4, 1.5, 0.2, 'setosa' ],\n",
        "[ 4.4, 2.9, 1.4, 0.2, 'setosa' ],\n",
        "[ 4.9, 3.1, 1.5, 0.1, 'setosa' ],\n",
        "[ 5.4, 3.7, 1.5, 0.2, 'setosa' ],\n",
        "[ 4.8, 3.4, 1.6, 0.2, 'setosa' ],\n",
        "[ 4.8, 3, 1.4, 0.1, 'setosa' ],\n",
        "[ 4.3, 3, 1.1, 0.1, 'setosa' ],\n",
        "[ 5.8, 4, 1.2, 0.2, 'setosa' ],\n",
        "[ 5.7, 4.4, 1.5, 0.4, 'setosa' ],\n",
        "[ 5.4, 3.9, 1.3, 0.4, 'setosa' ],\n",
        "[ 5.1, 3.5, 1.4, 0.3, 'setosa' ],\n",
        "[ 5.7, 3.8, 1.7, 0.3, 'setosa' ],\n",
        "[ 5.1, 3.8, 1.5, 0.3, 'setosa' ],\n",
        "[ 5.4, 3.4, 1.7, 0.2, 'setosa' ],\n",
        "[ 5.1, 3.7, 1.5, 0.4, 'setosa' ],\n",
        "[ 4.6, 3.6, 1, 0.2, 'setosa' ],\n",
        "[ 5.1, 3.3, 1.7, 0.5, 'setosa' ],\n",
        "[ 4.8, 3.4, 1.9, 0.2, 'setosa' ],\n",
        "[ 5, 3, 1.6, 0.2, 'setosa' ],\n",
        "[ 5, 3.4, 1.6, 0.4, 'setosa' ],\n",
        "[ 5.2, 3.5, 1.5, 0.2, 'setosa' ],\n",
        "[ 5.2, 3.4, 1.4, 0.2, 'setosa' ],\n",
        "[ 4.7, 3.2, 1.6, 0.2, 'setosa' ],\n",
        "[ 4.8, 3.1, 1.6, 0.2, 'setosa' ],\n",
        "[ 5.4, 3.4, 1.5, 0.4, 'setosa' ],\n",
        "[ 5.2, 4.1, 1.5, 0.1, 'setosa' ],\n",
        "[ 5.5, 4.2, 1.4, 0.2, 'setosa' ],\n",
        "[ 4.9, 3.1, 1.5, 0.1, 'setosa' ],\n",
        "[ 5, 3.2, 1.2, 0.2, 'setosa' ],\n",
        "[ 5.5, 3.5, 1.3, 0.2, 'setosa' ],\n",
        "[ 4.9, 3.1, 1.5, 0.1, 'setosa' ],\n",
        "[ 4.4, 3, 1.3, 0.2, 'setosa' ],\n",
        "[ 5.1, 3.4, 1.5, 0.2, 'setosa' ],\n",
        "[ 5, 3.5, 1.3, 0.3, 'setosa' ],\n",
        "[ 4.5, 2.3, 1.3, 0.3, 'setosa' ],\n",
        "[ 4.4, 3.2, 1.3, 0.2, 'setosa' ],\n",
        "[ 5, 3.5, 1.6, 0.6, 'setosa' ],\n",
        "[ 5.1, 3.8, 1.9, 0.4, 'setosa' ],\n",
        "[ 4.8, 3, 1.4, 0.3, 'setosa' ],\n",
        "[ 5.1, 3.8, 1.6, 0.2, 'setosa' ],\n",
        "# [ 4.6, 3.2, 1.4, 0.2, 'setosa' ],\n",
        "[ 5.3, 3.7, 1.5, 0.2, 'setosa' ],\n",
        "# [ 5, 3.3, 1.4, 0.2, 'setosa' ],\n",
        "[ 7, 3.2, 4.7, 1.4, 'versicolor' ],\n",
        "[ 6.4, 3.2, 4.5, 1.5, 'versicolor' ],\n",
        "[ 6.9, 3.1, 4.9, 1.5, 'versicolor' ],\n",
        "[ 5.5, 2.3, 4, 1.3, 'versicolor' ],\n",
        "[ 6.5, 2.8, 4.6, 1.5, 'versicolor' ],\n",
        "[ 5.7, 2.8, 4.5, 1.3, 'versicolor' ],\n",
        "[ 6.3, 3.3, 4.7, 1.6, 'versicolor' ],\n",
        "[ 4.9, 2.4, 3.3, 1, 'versicolor' ],\n",
        "[ 6.6, 2.9, 4.6, 1.3, 'versicolor' ],\n",
        "[ 5.2, 2.7, 3.9, 1.4, 'versicolor' ],\n",
        "[ 5, 2, 3.5, 1, 'versicolor' ],\n",
        "[ 5.9, 3, 4.2, 1.5, 'versicolor' ],\n",
        "[ 6, 2.2, 4, 1, 'versicolor' ],\n",
        "[ 6.1, 2.9, 4.7, 1.4, 'versicolor' ],\n",
        "[ 5.6, 2.9, 3.6, 1.3, 'versicolor' ],\n",
        "[ 6.7, 3.1, 4.4, 1.4, 'versicolor' ],\n",
        "[ 5.6, 3, 4.5, 1.5, 'versicolor' ],\n",
        "[ 5.8, 2.7, 4.1, 1, 'versicolor' ],\n",
        "[ 6.2, 2.2, 4.5, 1.5, 'versicolor' ],\n",
        "[ 5.6, 2.5, 3.9, 1.1, 'versicolor' ],\n",
        "[ 5.9, 3.2, 4.8, 1.8, 'versicolor' ],\n",
        "[ 6.1, 2.8, 4, 1.3, 'versicolor' ],\n",
        "[ 6.3, 2.5, 4.9, 1.5, 'versicolor' ],\n",
        "[ 6.1, 2.8, 4.7, 1.2, 'versicolor' ],\n",
        "[ 6.4, 2.9, 4.3, 1.3, 'versicolor' ],\n",
        "[ 6.6, 3, 4.4, 1.4, 'versicolor' ],\n",
        "[ 6.8, 2.8, 4.8, 1.4, 'versicolor' ],\n",
        "[ 6.7, 3, 5, 1.7, 'versicolor' ],\n",
        "[ 6, 2.9, 4.5, 1.5, 'versicolor' ],\n",
        "[ 5.7, 2.6, 3.5, 1, 'versicolor' ],\n",
        "[ 5.5, 2.4, 3.8, 1.1, 'versicolor' ],\n",
        "[ 5.5, 2.4, 3.7, 1, 'versicolor' ],\n",
        "[ 5.8, 2.7, 3.9, 1.2, 'versicolor' ],\n",
        "[ 6, 2.7, 5.1, 1.6, 'versicolor' ],\n",
        "[ 5.4, 3, 4.5, 1.5, 'versicolor' ],\n",
        "[ 6, 3.4, 4.5, 1.6, 'versicolor' ],\n",
        "[ 6.7, 3.1, 4.7, 1.5, 'versicolor' ],\n",
        "[ 6.3, 2.3, 4.4, 1.3, 'versicolor' ],\n",
        "[ 5.6, 3, 4.1, 1.3, 'versicolor' ],\n",
        "[ 5.5, 2.5, 4, 1.3, 'versicolor' ],\n",
        "[ 5.5, 2.6, 4.4, 1.2, 'versicolor' ],\n",
        "[ 6.1, 3, 4.6, 1.4, 'versicolor' ],\n",
        "[ 5.8, 2.6, 4, 1.2, 'versicolor' ],\n",
        "[ 5, 2.3, 3.3, 1, 'versicolor' ],\n",
        "[ 5.6, 2.7, 4.2, 1.3, 'versicolor' ],\n",
        "[ 5.7, 3, 4.2, 1.2, 'versicolor' ],\n",
        "[ 5.7, 2.9, 4.2, 1.3, 'versicolor' ],\n",
        "# [ 6.2, 2.9, 4.3, 1.3, 'versicolor' ],\n",
        "[ 5.1, 2.5, 3, 1.1, 'versicolor' ],\n",
        "# [ 5.7, 2.8, 4.1, 1.3, 'versicolor' ],\n",
        "[ 6.3, 3.3, 6, 2.5, 'virginica' ],\n",
        "[ 5.8, 2.7, 5.1, 1.9, 'virginica' ],\n",
        "[ 7.1, 3, 5.9, 2.1, 'virginica' ],\n",
        "[ 6.3, 2.9, 5.6, 1.8, 'virginica' ],\n",
        "[ 6.5, 3, 5.8, 2.2, 'virginica' ],\n",
        "[ 7.6, 3, 6.6, 2.1, 'virginica' ],\n",
        "[ 4.9, 2.5, 4.5, 1.7, 'virginica' ],\n",
        "[ 7.3, 2.9, 6.3, 1.8, 'virginica' ],\n",
        "[ 6.7, 2.5, 5.8, 1.8, 'virginica' ],\n",
        "[ 7.2, 3.6, 6.1, 2.5, 'virginica' ],\n",
        "[ 6.5, 3.2, 5.1, 2, 'virginica' ],\n",
        "[ 6.4, 2.7, 5.3, 1.9, 'virginica' ],\n",
        "[ 6.8, 3, 5.5, 2.1, 'virginica' ],\n",
        "[ 5.7, 2.5, 5, 2, 'virginica' ],\n",
        "[ 5.8, 2.8, 5.1, 2.4, 'virginica' ],\n",
        "[ 6.4, 3.2, 5.3, 2.3, 'virginica' ],\n",
        "[ 6.5, 3, 5.5, 1.8, 'virginica' ],\n",
        "[ 7.7, 3.8, 6.7, 2.2, 'virginica' ],\n",
        "[ 7.7, 2.6, 6.9, 2.3, 'virginica' ],\n",
        "[ 6, 2.2, 5, 1.5, 'virginica' ],\n",
        "[ 6.9, 3.2, 5.7, 2.3, 'virginica' ],\n",
        "[ 5.6, 2.8, 4.9, 2, 'virginica' ],\n",
        "[ 7.7, 2.8, 6.7, 2, 'virginica' ],\n",
        "[ 6.3, 2.7, 4.9, 1.8, 'virginica' ],\n",
        "[ 6.7, 3.3, 5.7, 2.1, 'virginica' ],\n",
        "[ 7.2, 3.2, 6, 1.8, 'virginica' ],\n",
        "[ 6.2, 2.8, 4.8, 1.8, 'virginica' ],\n",
        "[ 6.1, 3, 4.9, 1.8, 'virginica' ],\n",
        "[ 6.4, 2.8, 5.6, 2.1, 'virginica' ],\n",
        "[ 7.2, 3, 5.8, 1.6, 'virginica' ],\n",
        "[ 7.4, 2.8, 6.1, 1.9, 'virginica' ],\n",
        "[ 7.9, 3.8, 6.4, 2, 'virginica' ],\n",
        "[ 6.4, 2.8, 5.6, 2.2, 'virginica' ],\n",
        "[ 6.3, 2.8, 5.1, 1.5, 'virginica' ],\n",
        "[ 6.1, 2.6, 5.6, 1.4, 'virginica' ],\n",
        "[ 7.7, 3, 6.1, 2.3, 'virginica' ],\n",
        "[ 6.3, 3.4, 5.6, 2.4, 'virginica' ],\n",
        "[ 6.4, 3.1, 5.5, 1.8, 'virginica' ],\n",
        "[ 6, 3, 4.8, 1.8, 'virginica' ],\n",
        "[ 6.9, 3.1, 5.4, 2.1, 'virginica' ],\n",
        "[ 6.7, 3.1, 5.6, 2.4, 'virginica' ],\n",
        "[ 6.9, 3.1, 5.1, 2.3, 'virginica' ],\n",
        "[ 5.8, 2.7, 5.1, 1.9, 'virginica' ],\n",
        "[ 6.8, 3.2, 5.9, 2.3, 'virginica' ],\n",
        "[ 6.7, 3.3, 5.7, 2.5, 'virginica' ],\n",
        "[ 6.7, 3, 5.2, 2.3, 'virginica' ],\n",
        "[ 6.3, 2.5, 5, 1.9, 'virginica' ],\n",
        "# [ 6.5, 3, 5.2, 2, 'virginica' ],\n",
        "[ 6.2, 3.4, 5.4, 2.3, 'virginica' ],\n",
        "# [ 5.9, 3, 5.1, 1.8, 'virginica' ]\n",
        "]\n",
        "# Notify user of how many rows have been loaded\n",
        "print(\"Successfully loaded {0} rows!\".format(len(loaded_data)))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Successfully loaded 151 rows!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbQZ2-EAUpED",
        "colab_type": "text"
      },
      "source": [
        "After loading the dataset into our script, it is also necessary to configure it properly to train on the right attributes for the right target."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmynFDPoVxsE",
        "colab_type": "code",
        "outputId": "2ad7bff3-a603-4af6-fbd2-65cdffcbb88b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Define the attributes to be used for testing (this provides\n",
        "# extra freedom to the user instead of editing their dataset\n",
        "# many times - and also if there are any IDs)\n",
        "\n",
        "# In this scenario we want to use all our attributes\n",
        "#attributes = [ 'Outlook', 'Temp', 'Humidity', 'Windy', 'Play' ]\n",
        "attributes = ['sepal_length','sepal_width','petal_length','petal_width','species']\n",
        "\n",
        "# We also need to define the class for the script to target\n",
        "#target = \"Play\"\n",
        "target = \"species\"\n",
        "\n",
        "print (\"There are {0} attributes to train on and we are targeting \\\"{1}\\\"\".format(len(attributes), target))"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 5 attributes to train on and we are targeting \"species\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCJbuR4heZT-",
        "colab_type": "text"
      },
      "source": [
        "Now with the updated configuration, our script needs to apply the necessary changes to our datasets to ensure the right columns and rows are used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "soQxrFBEeZoA",
        "colab_type": "code",
        "outputId": "c36ed441-9fbd-4233-b209-0e679552557b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# In our example dataset, the first row had no actual data\n",
        "# instead it was used to title each column.  Lets separate\n",
        "# them into two different objects for use later\n",
        "\n",
        "header_row = loaded_data[0]\n",
        "training_rows = loaded_data[1:]\n",
        "\n",
        "# I come from a web developer background so using a json \n",
        "# object seems more familiar with me \n",
        "jsond_rows = []\n",
        "for i in range(0, len(training_rows)):\n",
        "    json_data = {}\n",
        "    row = training_rows[i]\n",
        "    for j in range(0, len(header_row)):\n",
        "        json_data[header_row[j]] = row[j]\n",
        "\n",
        "    jsond_rows.append(json_data)\n",
        "\n",
        "# Now we need to remove any columns that won't be used by the\n",
        "# algorithm when training, there's probably a really easy\n",
        "# way to map this properly but I haven't gone through the \n",
        "# documentation thoroughly enough\n",
        "for i in range(0, len(jsond_rows)):\n",
        "    json_data = {}\n",
        "    row = jsond_rows[i]\n",
        "    for j in range(0, len(attributes)):\n",
        "        json_data[attributes[j]] = row[attributes[j]]\n",
        "\n",
        "    jsond_rows[i] = json_data\n",
        "\n",
        "# Remove the target from the attributes so we don't get mixed up later\n",
        "if target in attributes:\n",
        "    attributes.remove(target)   \n",
        "\n",
        "print (\"We've filtered out any unused attributes!\")"
      ],
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We've filtered out any unused attributes!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNq6w-VUmAOp",
        "colab_type": "text"
      },
      "source": [
        "We also need to define extra utility functions to assist us instead of reusing code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74MBhEtHmEkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def getUniqueValues(data, attributeName):\n",
        "    # Get all the possible values for our attribute\n",
        "    attributeValues = []\n",
        "    for row in data:\n",
        "        val = row[attributeName]\n",
        "        if val not in attributeValues:\n",
        "            attributeValues.append(val)\n",
        "\n",
        "    return attributeValues\n",
        "\n",
        "# https://www.geeksforgeeks.org/python-find-most-frequent-element-in-a-list/\n",
        "def most_frequent(List): \n",
        "    dict = {} \n",
        "    count, itm = 0, '' \n",
        "    for item in reversed(List): \n",
        "        dict[item] = dict.get(item, 0) + 1\n",
        "        if dict[item] >= count : \n",
        "            count, itm = dict[item], item \n",
        "    return(itm) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejnYcB8dSK-e",
        "colab_type": "text"
      },
      "source": [
        "Next we need to define the functions used to calcualate entropy and information gain. \n",
        "\n",
        "Entropy can be easily calculated using the formula above and using the occurrences of each value within in its column as seen in the code snippet below.\n",
        "\n",
        "In the code snippet, the function calculates probability/occurrence of a value from the target column and then uses that value to calculate the entropy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HlxBCTULEgNX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# To calculate entropy we basically \n",
        "# need to calculate a value's occurence\n",
        "# within it's own column/class\n",
        "# the data object is still the whole set\n",
        "# and the attribute name is the attribute\n",
        "# for which we are calulcating entropy for\n",
        "\n",
        "def calculateEntropy(data, attributeName):\n",
        "    # Used to as a denominator in the fraction\n",
        "    numberOfRows = len(data)\n",
        "\n",
        "    # This small patch of code basically counts\n",
        "    # the occurrences of a certain value within\n",
        "    # that column\n",
        "    occurrs = {}\n",
        "    for row in data: \n",
        "        val = row[attributeName]\n",
        "\n",
        "        if val not in occurrs:\n",
        "            occurrs[val] = 0\n",
        "    \n",
        "        occurrs[val] += 1\n",
        "\n",
        "    entropy = 0\n",
        "    for val in occurrs:\n",
        "        # Calculates proportions\n",
        "        # Float is needed here otherwise it becomes 0\n",
        "        p = occurrs[val] / float(numberOfRows)\n",
        "        entropy += - p * math.log(p, 2)\n",
        "\n",
        "    return entropy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyo-pnMXP9IO",
        "colab_type": "text"
      },
      "source": [
        "Information Gain on the other hand is a bit more complex requiring multiple entropy calculations. It requires the grouping of values from another attribute and then calculating their individual entropies in relation to the target attribute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2H_mSNZKtSR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pass the target Entropy, data and attribute name for which\n",
        "# you want to calculate the information gain for\n",
        "def calculateIGain(data, target, attributeName):\n",
        "    # Used to as a denominator in the fraction\n",
        "    numberOfRows = len(data)\n",
        "\n",
        "    # We need the target's entropy to calculate info gain\n",
        "    targetEntropy = calculateEntropy(data, target)\n",
        "\n",
        "    # Get all the possible values for our attribute\n",
        "    attributeValues = getUniqueValues(data, attributeName)\n",
        "\n",
        "    # We want to group each attribute \n",
        "    # and then calculate their entropies\n",
        "    sumOfEntropies = 0\n",
        "    for attribute in attributeValues:\n",
        "        # This function is used to group rows together based on the\n",
        "        # value of an attribute\n",
        "        def isGroup(row):\n",
        "            if row[attributeName] == attribute:\n",
        "                return True\n",
        "            else:\n",
        "                return False\n",
        "\n",
        "        # Groups the rows\n",
        "        matchedRows = list(filter(isGroup, data))\n",
        "\n",
        "        entropy = len(matchedRows) / float(numberOfRows) * calculateEntropy(matchedRows, target)\n",
        "        sumOfEntropies += entropy\n",
        "\n",
        "    return targetEntropy - sumOfEntropies"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5xId8vuPvhK",
        "colab_type": "text"
      },
      "source": [
        "Due to the way decision trees treat information gain, it is better to sort all the different attributes and features first with their information gain metrics before creating their nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u55Yrr6QX9RP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This function gets the attribute with the biggest information gain\n",
        "def getBiggestInfoGain(infoGain):\n",
        "    attributeName = None\n",
        "    biggestGain = None\n",
        "\n",
        "    for attribute in infoGain.keys():\n",
        "        if biggestGain is None or biggestGain < infoGain[attribute]:\n",
        "            biggestGain = infoGain[attribute]\n",
        "            attributeName = attribute \n",
        "\n",
        "    return attributeName\n",
        "\n",
        "def getInfoGains(data, target, attributes):\n",
        "    # Let's find out how much information gain each attribute has\n",
        "    informationGain = {}\n",
        "    for attr in attributes:\n",
        "        # Skip the target attr\n",
        "        if attr == target:\n",
        "            continue\n",
        "        iGain = calculateIGain(data, target, attr)\n",
        "        informationGain[attr] = iGain\n",
        "    \n",
        "    return informationGain"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVzNaG11bmUb",
        "colab_type": "text"
      },
      "source": [
        "Now we can actually get on to creating the decision tree. Our decision tree will have three variables at each stage: attributeName, children and result. The attribute name is what is currently being tested, the children are any further stages and the result appears when the tree ends.\n",
        "\n",
        "To create the decision tree, our script needs to filter the data at every stage and create a funnel until an answer can be obtained. This will require many loops of the create Tree function as seen below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MeYnliGej5t6",
        "colab_type": "code",
        "outputId": "a09c6a8a-e911-4828-cb20-8bf9cfb6f00c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def createTree(data, target, attributes):\n",
        "    # Create the tree object (this could also be a child)\n",
        "    tree = {}\n",
        "\n",
        "    # Get all the possible values for our target\n",
        "    targetValues = getUniqueValues(data, target)\n",
        "\n",
        "    # If there is only one possible value we can\n",
        "    # end the tree\n",
        "    if len(targetValues) == 1:\n",
        "        tree['result'] = targetValues[0]\n",
        "        return tree\n",
        "\n",
        "    # If there are no attributes to train with\n",
        "    # use the most common one\n",
        "    if len(attributes) == 0:\n",
        "        tree['result'] = most_frequent(targetValues)\n",
        "        return tree\n",
        "\n",
        "    # Calculate info gains for all the attributes\n",
        "    infoGains = getInfoGains(data, target, attributes)\n",
        "    # Get the biggest one \n",
        "    biggestInfoGain = getBiggestInfoGain(infoGains)\n",
        "\n",
        "    # Set the attribute being tested at this stage\n",
        "    tree['attributeName'] = biggestInfoGain\n",
        "    tree['children'] = {}\n",
        "\n",
        "    # Remove the attribute now\n",
        "    updatedAttributes = attributes.copy() # using copy doesn't change the attributes variable in the global\n",
        "    updatedAttributes.remove(biggestInfoGain)\n",
        "\n",
        "    # Get all the possible values of the biggest info attribute\n",
        "    possibleValues = getUniqueValues(data, biggestInfoGain)\n",
        "\n",
        "    # For every possible value we need to create a branch \n",
        "    # with a more filtered data set and the remaining attributes\n",
        "    for value in possibleValues:\n",
        "        filteredData = []\n",
        "        for row in data:\n",
        "            if row[biggestInfoGain] == value:\n",
        "                filteredData.append(row)\n",
        "\n",
        "        tree['children'][value] = createTree(filteredData, target, updatedAttributes)\n",
        "\n",
        "    return tree\n",
        "\n",
        "decisionTree = createTree(jsond_rows, target, attributes)\n",
        "\n",
        "print (decisionTree)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attributeName': 'petal_length', 'children': {1.4: {'result': 'setosa'}, 1.3: {'result': 'setosa'}, 1.5: {'result': 'setosa'}, 1.7: {'result': 'setosa'}, 1.6: {'result': 'setosa'}, 1.1: {'result': 'setosa'}, 1.2: {'result': 'setosa'}, 1: {'result': 'setosa'}, 1.9: {'result': 'setosa'}, 4.7: {'result': 'versicolor'}, 4.5: {'attributeName': 'sepal_length', 'children': {6.4: {'result': 'versicolor'}, 5.7: {'result': 'versicolor'}, 5.6: {'result': 'versicolor'}, 6.2: {'result': 'versicolor'}, 6: {'result': 'versicolor'}, 5.4: {'result': 'versicolor'}, 4.9: {'result': 'virginica'}}}, 4.9: {'attributeName': 'sepal_width', 'children': {3.1: {'result': 'versicolor'}, 2.5: {'result': 'versicolor'}, 2.8: {'result': 'virginica'}, 2.7: {'result': 'virginica'}, 3: {'result': 'virginica'}}}, 4: {'result': 'versicolor'}, 4.6: {'result': 'versicolor'}, 3.3: {'result': 'versicolor'}, 3.9: {'result': 'versicolor'}, 3.5: {'result': 'versicolor'}, 4.2: {'result': 'versicolor'}, 3.6: {'result': 'versicolor'}, 4.4: {'result': 'versicolor'}, 4.1: {'result': 'versicolor'}, 4.8: {'attributeName': 'sepal_length', 'children': {5.9: {'result': 'versicolor'}, 6.8: {'result': 'versicolor'}, 6.2: {'result': 'virginica'}, 6: {'result': 'virginica'}}}, 4.3: {'result': 'versicolor'}, 5: {'attributeName': 'sepal_length', 'children': {6.7: {'result': 'versicolor'}, 5.7: {'result': 'virginica'}, 6: {'result': 'virginica'}, 6.3: {'result': 'virginica'}}}, 3.8: {'result': 'versicolor'}, 3.7: {'result': 'versicolor'}, 5.1: {'attributeName': 'sepal_length', 'children': {6: {'result': 'versicolor'}, 5.8: {'result': 'virginica'}, 6.5: {'result': 'virginica'}, 6.3: {'result': 'virginica'}, 6.9: {'result': 'virginica'}, 5.9: {'result': 'virginica'}}}, 3: {'result': 'versicolor'}, 6: {'result': 'virginica'}, 5.9: {'result': 'virginica'}, 5.6: {'result': 'virginica'}, 5.8: {'result': 'virginica'}, 6.6: {'result': 'virginica'}, 6.3: {'result': 'virginica'}, 6.1: {'result': 'virginica'}, 5.3: {'result': 'virginica'}, 5.5: {'result': 'virginica'}, 6.7: {'result': 'virginica'}, 6.9: {'result': 'virginica'}, 5.7: {'result': 'virginica'}, 6.4: {'result': 'virginica'}, 5.4: {'result': 'virginica'}, 5.2: {'result': 'virginica'}}}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdkI5LVIQJKl",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "As I was still new to Python there were a lot of short cuts and bad coding practices in this algorithm. Ultimately the algorithm does work in building an ID3 decision tree model however it is yet to be tested.\n",
        "\n",
        "To evaluate the performance of our algorithm we will be using the iris dataset which has already been previously loaded and also a new function to predict individual results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQOx-0_YwHiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "77b9ea8b-f21f-46cd-db8a-857a977f334c"
      },
      "source": [
        "def predict(data):\n",
        "    tree = decisionTree\n",
        "    while 'result' not in tree:\n",
        "        child = data[tree['attributeName']]\n",
        "        tree = tree['children'][child]\n",
        "\n",
        "    return tree['result']\n",
        "\n",
        "# This test data set will only work if you are using the iris dataset\n",
        "# [ 'sepal_length','sepal_width','petal_length','petal_width','species' ],\n",
        "test_data = [\n",
        "    { 'sepal_length': 4.6, 'sepal_width': 3.2, 'petal_length': 1.4, 'petal_width': 0.2 }, # Setosa\n",
        "    { 'sepal_length': 5, 'sepal_width': 3.3, 'petal_length': 1.4, 'petal_width': 0.2 }, # Setosa\n",
        "    { 'sepal_length': 6.2, 'sepal_width': 2.9, 'petal_length': 4.3, 'petal_width': 1.3 }, # versicolor\n",
        "    { 'sepal_length': 5.7, 'sepal_width': 2.8, 'petal_length': 4.1, 'petal_width': 1.3 }, # versicolor\n",
        "    { 'sepal_length': 6.5, 'sepal_width': 3, 'petal_length': 5.2, 'petal_width': 2 }, # virginica\n",
        "    { 'sepal_length': 5.9, 'sepal_width': 3, 'petal_length': 5.1, 'petal_width': 1.8 }, # virginica\n",
        "]\n",
        "\n",
        "for row in test_data:\n",
        "    prediction = predict(row)\n",
        "    print (row, prediction)\n"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'sepal_length': 4.6, 'sepal_width': 3.2, 'petal_length': 1.4, 'petal_width': 0.2} setosa\n",
            "{'sepal_length': 5, 'sepal_width': 3.3, 'petal_length': 1.4, 'petal_width': 0.2} setosa\n",
            "{'sepal_length': 6.2, 'sepal_width': 2.9, 'petal_length': 4.3, 'petal_width': 1.3} versicolor\n",
            "{'sepal_length': 5.7, 'sepal_width': 2.8, 'petal_length': 4.1, 'petal_width': 1.3} versicolor\n",
            "{'sepal_length': 6.5, 'sepal_width': 3, 'petal_length': 5.2, 'petal_width': 2} virginica\n",
            "{'sepal_length': 5.9, 'sepal_width': 3, 'petal_length': 5.1, 'petal_width': 1.8} virginica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEWhiFAp0s7x",
        "colab_type": "text"
      },
      "source": [
        "Although there wasn't extensive testing, the results of the above is sufficient to prove that the model is working to expectations. In terms of performance, parsing the data into a JSON object would require a lot of computing power especially for bigger datasets, so it would be more efficient to stick with the array format especially since that is the default format used when importaing data from CSV files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ByBK8BVQLgJ",
        "colab_type": "text"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "To conclude, the ID3 decision tree is a lot more easier to comprehend once it has been built out in code. In this form, it is simple to understand the creation and methodologies behind the decision tree as well as the formulas used. To better this script, use of more libraries would help significantly as many functions above could have easily been replaced and better optimized. Datasets could have also been imported directly from libraries like scikit instead of being initialized directly into the objects. Overall it has been a great experience to learn Python and a hands on experience with the ID3 algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpNV9xA5QNKk",
        "colab_type": "text"
      },
      "source": [
        "# Ethics\n",
        "\n",
        "The ID3 algorithm is an incredibly useful and powerful tool when used correctly. With the wrong type of data the ID3 algorithm could be skewed and biased which may be detrimental depending on its use cases. One example is if users were to feed an algorithm with disproportional crime data. This would create a skew in the algorithm and incorrectly predict outcomes. Also as noted in the code above, when there is not enough information, the ID3 algorithm falls back onto the most common outcome which can be fatal decision. However, despite all these disadvantages, these hurdles must be tested and overcome for a future with better machine learning algorithms and artificial intelligence for the greater good."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6y8sfLIQOPB",
        "colab_type": "text"
      },
      "source": [
        "# Links\n",
        "\n",
        "https://www.youtube.com/watch?v=HeMYgsp6Cqo&feature=youtu.be\n",
        "\n",
        "https://github.com/13286733-u/MLA2\n",
        "\n",
        "https://colab.research.google.com/drive/16E_NCBxcmSxP-ya0vR4sB5Icc93C_fnQ\n"
      ]
    }
  ]
}